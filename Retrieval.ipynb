{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Notebook\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "import random\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pickle variables (Language model, lyrics database)\n",
    "### defined in Preprocessing Notebook\n",
    "##### Download data from https://www.dropbox.com/s/h94ebpetwmw45sp/Data.zip?dl=0\n",
    "Loading will take ~1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading Language models + relevant data\")\n",
    "start = time.time()\n",
    "print('-----------------------------------------------------')\n",
    "with open('Data/inverted_terms', 'rb') as f:\n",
    "    inverted_terms = pickle.load(f)\n",
    "print('loaded: inverted_terms-------------------------------')\n",
    "\n",
    "with open('Data/inverted_bigrams', 'rb') as f:\n",
    "    inverted_bigrams = pickle.load(f)\n",
    "print('=====loaded: inverted_bigrams------------------------')\n",
    "\n",
    "with open('Data/song_total_terms', 'rb') as f:\n",
    "    song_total_terms = pickle.load(f)\n",
    "print('==========loaded: song_total_terms-------------------')\n",
    "\n",
    "with open('Data/song_total_bigrams', 'rb') as f:\n",
    "    song_total_bigrams = pickle.load(f)\n",
    "print('===============loaded: song_total_bigrams------------')\n",
    "\n",
    "with open('Data/lyrics_original', 'rb') as f:\n",
    "    lyrics_original = pickle.load(f) #lyrics with capitalisation / punctuation\n",
    "print('======================loaded: lyrics_original--------')\n",
    "\n",
    "with open('Data/tokenized_lyrics', 'rb') as f:\n",
    "    tokenized_lyrics = pickle.load(f) #stemmed, tokenized lyrics (list of lists of terms)\n",
    "print('=============================loaded: tokenized_lyrics')\n",
    "print('=====================================================')\n",
    "\n",
    "collection_N = sum([e[0] for e in inverted_terms.values()])\n",
    "bigram_collection_N = sum([e[0] for e in inverted_bigrams.values()])\n",
    "\n",
    "end = time.time()\n",
    "print(\"All data loaded in {:0>2d}:{:0>2.0f}\".format(int((end-start)//60), (end-start)%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_1(query, max_heap_size=100, smoothing=.85):\n",
    "    '''\n",
    "    base: inverted unigram language model\n",
    "    Retrieves 'max_heap_size' indices of the lyrics in the lyrics DataFrame in decreasing order of \n",
    "    relevance to the query\n",
    "    uses an inverted unigram language model defined in the preprocessing notebook \n",
    "    applies smoothing with smoothing factor defined by 'smoothing'\n",
    "    uses only unigrams\n",
    "    '''\n",
    "    stemmer = PorterStemmer() #stemmer\n",
    "    #max_heap is a list with (score, index) tuples, where the index is the index of the song in the dataframe\n",
    "    max_heap = list(zip(np.repeat(0, max_heap_size), np.repeat(-1, max_heap_size))) #initialise max heap with (0, -1)\n",
    "    query = re.sub('\\W', ' ', query) #strip query from punctuation\n",
    "    terms = [stemmer.stem(t) for t in word_tokenize(query.lower())] #stemmed, tokenized query terms\n",
    "    \n",
    "    scores = {} #{song index : score}\n",
    "    for term in terms: #sum over query terms:\n",
    "        if term in inverted_terms: #if term in the collection:\n",
    "            collection_count = inverted_terms[term][0]\n",
    "            for key in inverted_terms[term][1]: #for each document in which term occurs:\n",
    "                #add term score (proportional to TF, inversely proportional to IDF) to total score of document\n",
    "                score = np.log(1+ ((1-smoothing)*inverted_terms[term][1][key]/song_total_terms[key])\\\n",
    "                                                /((smoothing*collection_count)/collection_N))\n",
    "                if key in scores:\n",
    "                    scores[key] += score\n",
    "                else:\n",
    "                    scores[key] = score\n",
    "    for key in scores:\n",
    "        scores[key] += np.log(song_total_terms[key])/np.log(collection_N) #length prior\n",
    "        if scores[key] > min(max_heap)[0]: #update max heap\n",
    "            heapq.heappop(max_heap)\n",
    "            heapq.heappush(max_heap, (scores[key],key))\n",
    "    \n",
    "    return [x[1] for x in sorted(max_heap, reverse=True)] #return indices of songs in decreasing order of relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_2(query, max_heap_size=100, smoothing=.85): \n",
    "    '''\n",
    "    base: bigram language model with inverted file index\n",
    "    same as retrieve_1 but uses bigrams\n",
    "    '''\n",
    "    stemmer = PorterStemmer() #stemmer\n",
    "    #max_heap is a list with (score, index) tuples, where the index is the index of the song in the dataframe\n",
    "    max_heap = list(zip(np.repeat(0, max_heap_size), np.repeat(-1, max_heap_size))) #initialise max heap with (0, -1)\n",
    "    query = re.sub('\\W', ' ', query) #strip query from punctuation\n",
    "    terms = [stemmer.stem(t) for t in word_tokenize(query.lower())] #stemmed, tokenized query terms\n",
    "    query_bigrams = list(nltk.bigrams(terms))\n",
    "    \n",
    "    scores = {} #{song index : score}\n",
    "    for bigram in query_bigrams: #sum over query bigrams:\n",
    "        if bigram in inverted_bigrams: #if bigram in the collection:\n",
    "            collection_count = inverted_bigrams[bigram][0]\n",
    "            for key in inverted_bigrams[bigram][1]: #for each document in which bigram occurs:\n",
    "                #add bigram score (proportional to TF, inversely proportional to IDF) to total score of document\n",
    "                score = np.log(1+ ((1-smoothing)*inverted_bigrams[bigram][1][key]/song_total_bigrams[key])\\\n",
    "                                                /((smoothing*collection_count)/bigram_collection_N))\n",
    "                if key in scores:\n",
    "                    scores[key] += score\n",
    "                else:\n",
    "                    scores[key] = score\n",
    "    for key in scores:\n",
    "        scores[key] += np.log(song_total_bigrams[key])/np.log(bigram_collection_N) #length prior\n",
    "        if scores[key] > min(max_heap)[0]: #update max heap\n",
    "            heapq.heappop(max_heap)\n",
    "            heapq.heappush(max_heap, (scores[key],key))\n",
    "            \n",
    "    return [x[1] for x in sorted(max_heap, reverse=True)] #return indices of songs in decreasing order of relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerank method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_ngrams(indices, query, n=2): \n",
    "    '''\n",
    "    rerank indices on ngram presence\n",
    "    defines ngrams on the fly using nltk.ngrams with n = n\n",
    "    this is rather quick since we have the list with terms for each song (tokenized_lyrics) and we only need to rerank\n",
    "    max_heap_size indices (from retrieval method)\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    query = re.sub('\\W', ' ', query) #strip query from punctuation\n",
    "    terms = [stemmer.stem(t) for t in word_tokenize(query.lower())] #stemmed, tokenized query terms\n",
    "    query_ngrams = list(nltk.ngrams(terms, n)) #all ngrams generated from query\n",
    "    \n",
    "    ngram_counts = [] #for each index in indices, (-) the number of query ngrams that occur in those lyrics\n",
    "    for index in indices:\n",
    "        if index == -1: # empty result\n",
    "            ngram_counts.append(1) #this is worse than 0\n",
    "            break\n",
    "        #list of lists of ngrams for each lyrics in the top results\n",
    "        all_ngrams = list(nltk.ngrams(tokenized_lyrics[index], n)) #all ngrams for lyrics at index\n",
    "        count = 0\n",
    "        for ngram in query_ngrams: #for every ngram from the query\n",
    "            if ngram in all_ngrams: #if the nrgram is present in the lyrics: improve score\n",
    "                count -= 1 #minus for sorting to remain order within the same counts (reverse sort reverses this)\n",
    "\n",
    "        ngram_counts.append(count)\n",
    "    \n",
    "    #sort indices based on ngram_counts\n",
    "    new_ranking = [index for _,index in sorted(zip(ngram_counts,indices), key=lambda x:x[0])]\n",
    "    return new_ranking #indices from the lyrics dataframe in decreasing order of relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proxy query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get a query proxy: a string of k consecutive words extracted randomly from a random song\n",
    "these queries are used to test the models and tweak hyperparameters\n",
    "'''\n",
    "def query_proxy(k=6):\n",
    "    words=[]\n",
    "    while(len(words)<k): #make sure the song has at least k words\n",
    "        sample = lyrics_original.sample()\n",
    "        words = sample['lyrics'].iloc[0].split() #split words\n",
    "\n",
    "    r = random.randint(0,len(words)-k) #random location in the song lyrics\n",
    "    query = ' '.join(words[r:r+k]) #slice k words from lyrics\n",
    "\n",
    "    print('Query: ', query)\n",
    "    print(\"Index: {}\\nSong: {}\\nArtist: {}\".format(sample['index'].iloc[0], sample['song'].iloc[0],\\\n",
    "                                                   sample['artist'].iloc[0]))\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the ranked results from a query for all models \n",
    "#### results = indices of songs in the DataFrame\n",
    "models: <br>\n",
    "unigram model <br>\n",
    "unigram model + bigram rerank<br>\n",
    "bigram model<br>\n",
    "bigram model + trigram rerank<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "#################### DEFINE QUERY HERE OR GET A PROXY ####################\n",
    "##########################################################################\n",
    "query = \"All I wanna hear her say is are you mine\"\n",
    "#query = query_proxy()\n",
    "##########################################################################\n",
    "\n",
    "max_heap_size = 100\n",
    "\n",
    "print(\"\\nGetting results...\")\n",
    "#unigram model\n",
    "start = time.time()\n",
    "indices_uni = retrieve_1(query, max_heap_size) #unigram model\n",
    "uni_time = time.time()-start\n",
    "print(\"time for unigram model results: \\t\\t{} seconds\".format(uni_time))\n",
    "\n",
    "#unigram model + bigram rerank\n",
    "start = time.time()\n",
    "indices_uni_bi = rerank_ngrams(indices_uni, query, 2) #rerank indices on bigrams\n",
    "uni_bi_time = time.time()-start\n",
    "print(\"time to rerank unigram results on bigrams: \\t{} seconds\".format(uni_bi_time))\n",
    "\n",
    "#bigram model\n",
    "start = time.time()\n",
    "indices_bi = retrieve_2(query, max_heap_size) #bigram model\n",
    "bi_time = time.time()-start\n",
    "print(\"time for bigram model results: \\t\\t\\t{} seconds\".format(bi_time))\n",
    "\n",
    "#bigram model + trigram rerank\n",
    "start = time.time()\n",
    "indices_bi_tri = rerank_ngrams(indices_bi, query, 3) #rerank indices on trigrams\n",
    "bi_tri_time = time.time()-start\n",
    "print(\"time to rerank bigram results on trigrams: \\t{} seconds\".format(bi_tri_time))\n",
    "\n",
    "indices = [indices_uni, indices_uni_bi, indices_bi, indices_bi_tri]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print result page for each model (top 'nr_results_to_show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nr_results_to_show = 20\n",
    "\n",
    "names = ['\\033[1mmixed model + trigram rerank\\033[0m', '\\033[1munigram model\\033[0m', \\\n",
    "         '\\033[1munigram model + bigram rerank\\033[0m', '\\033[1mbigram model\\033[0m',\\\n",
    "         '\\033[1mbigram model + trigram rerank\\033[0m']\n",
    "i = 0\n",
    "for result_page in indices:\n",
    "    print(names[i])\n",
    "    #print(result_page)\n",
    "    i+=1\n",
    "    rank=1\n",
    "    for index in result_page[:min(nr_results_to_show, len(result_page))]:\n",
    "        if(index==-1): #no more songs indexed\n",
    "            break\n",
    "        print(\"Rank: {}  Song: {}\\tArtist: {}\\tIndex: {}\".format(rank, lyrics_original['song'].iloc[index], \\\n",
    "                                                      lyrics_original['artist'].iloc[index], index))\n",
    "        rank+=1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous timing study results (MacBook Pro 2018 8GB ram T2 Intel Core i5)\n",
    "\n",
    "times_unigram = [0.6614131117, 0.9135156918, 1.36761508, 1.556321063, 1.612079, 1.63472198, 1.936166196, 2.028390484, \\\n",
    "2.588905816, 2.428667722, 2.679294305, 2.637917728, 2.530018735]\n",
    "times_bigram = [0.07451394796, 0.1133789158, 0.1427968192, 0.1722764993, 0.265690732, 0.2704293275, 0.3036595631, \\\n",
    "0.371507051, 0.3504348707, 0.4463950658, 0.4693552208, 0.4954117918]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(10, 6))\n",
    "matplotlib.rc('xtick', labelsize=17) \n",
    "matplotlib.rc('ytick', labelsize=17) \n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "plt.plot(range(1,14), times_unigram)\n",
    "plt.plot(range(2,14), times_bigram)\n",
    "plt.xlabel(\"nr of query terms\")\n",
    "plt.ylabel(\"seconds until result\")\n",
    "plt.legend((\"unigram model*\",\"bigram model**\"))\n",
    "plt.xscale('linear')\n",
    "plt.xticks(range(1,14));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Are you sure you want to do the timing study again?\").lower() == 'yes':\n",
    "    times_unigram = []\n",
    "    for nr_query_terms in range (1,14):\n",
    "        print(\"queries with {} term(s)\".format(nr_query_terms))\n",
    "        seconds = []\n",
    "        for iterations in range(50):\n",
    "            query = query_proxy(nr_query_terms)\n",
    "            start = time.time()\n",
    "            x = retrieve_invert_uni(query, 100)\n",
    "            seconds.append(time.time()-start)\n",
    "        average = np.mean(seconds)\n",
    "        print(\"average time: {}\".format(average))\n",
    "        times_unigram.append(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Are you sure you want to do the timing study again?\").lower() == 'yes':\n",
    "    times_bigram = []\n",
    "    for nr_query_terms in range (2,14):\n",
    "        print(\"queries with {} term(s)\".format(nr_query_terms))\n",
    "        seconds = []\n",
    "        for iterations in range(100):\n",
    "            query = query_proxy(nr_query_terms)\n",
    "            start = time.time()\n",
    "            x = retrieve_invert_bi(query, 100)\n",
    "            seconds.append(time.time()-start)\n",
    "        average = np.mean(seconds)\n",
    "        print(\"average time: {}\".format(average))\n",
    "        times_bigram.append(average)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
