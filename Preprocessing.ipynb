{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of data + Language Model creation\n",
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import bigrams\n",
    "import nltk\n",
    "import queue\n",
    "from threading import Thread\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and filter, then save pickle\n",
    "each model or important dataset will be saved as a pickle file to load (more quickly than creating it) in the retrieval part (310 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv('lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = lyrics.drop(['year', 'genre'], axis=1) #Don't need the year or genre data\n",
    "lyrics['song'] = lyrics['song'].str.replace('-', ' '); #song names are spaced with dashes\n",
    "lyrics['artist'] = lyrics['artist'].str.replace('-', ' '); #artist names are spaced with dashes\n",
    "lyrics = lyrics[lyrics.song != ''] #remove empty song title\n",
    "lyrics = lyrics[lyrics.lyrics != ''] #remove empty lyrics\n",
    "lyrics = lyrics.drop(['index'], axis=1) #drop index column\n",
    "lyrics = lyrics.dropna(how='any').reset_index() #drop empty data rows\n",
    "lyrics1 = lyrics.copy()\n",
    "lyrics['lyrics'] = lyrics['lyrics'].str.replace(r'\\W', ' ', regex=True) #punctuation to whitespace\n",
    "lyrics['lyrics'] = lyrics['lyrics'].str.lower() #lower case lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and stem lyrics (then save pickle)\n",
    "This results in a list containing a list for every song with the stemmed words in that song (800MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lyrics = lyrics['lyrics'].apply(word_tokenize) #tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "tokenized_lyrics = [[stemmer.stem(w) for w in song] for song in tokenized_lyrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unigram Language Model for each song from the tokenized lyrics and save\n",
    "This results in a list of dictionaries. Each dictionary belongs to a song and contains counts of each term. To get the sampled probability of a term call .freq(term) (400 MB) From this we will later create an inverted index (may be a detour but we only thought of inverted index after this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDistList = [FreqDist(terms) for terms in tokenized_lyrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the list of dictionaries into one to create the unigram Language Model for the collection and save\n",
    "This is quite an expensive operation but still doable like this because the number of unique terms is not crazy big. Results in one dictionary with all terms of all lyrics with their counts in the collection (7 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = Counter({})\n",
    "for i in range(len(FreqDictList)):\n",
    "    collection+=FreqDistList[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = dict(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bigrams for each song\n",
    "This results in the same structure as tokenized_lyrics (3 steps above) but containing bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_bigrams = [list(nltk.bigrams(x)) for x in tokenized_lyrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bigram Language Model for each song from the lyrics bigrams and save\n",
    "Creates the list of dictionaries (one for each song) containing the bigrams and their counts (1.12 GB) From this we will later create an inverted index (may be a detour but we only thought of inverted index after this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freqlist = [FreqDist(bigrams) for bigrams in lyrics_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the list of bigram dictionairies into one to create the bigram Language Model for the collection and save\n",
    "This is now a very expensive operation because there exist many many unique bigrams, to merge dictionaries means to check for each key whether it exists in the other dictionary before adding to the value or creating a new key, therefore growing in cost in the length of dictionaries to merge and also the number of dictionaries. To counter this I split the list of bigram language models recursively and started a few threads to work on it. This is likely not the most efficient way but it worked. (150 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = queue.Queue() #queue to contain intermediate results from threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all the dictionaries in the list_of_dictionaries into one and put it in the queue (for easy synchronization)\n",
    "def merge_dicts(list_of_dicts):\n",
    "    result = Counter({})\n",
    "    for d in list_of_dicts:\n",
    "        result+=d\n",
    "    queue.put(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split a list in half\n",
    "def split_list(l):\n",
    "    return l[:len(l)//2], l[len(l)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "#split the list of bigram models recursively and add each job to the list of threads\n",
    "def recursive_threads(dict_list, factor=4): #32 threads\n",
    "    l1, l2 = split_list(dict_list)\n",
    "    if factor > 0:\n",
    "        recursive_threads(l1, factor-1)\n",
    "        recursive_threads(l2, factor-1)\n",
    "    else:\n",
    "        threads.append(Thread(target=merge_dicts, args=(l1,)))\n",
    "        threads.append(Thread(target=merge_dicts, args=(l2,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_threads(bigram_freqlist) #Create jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#start threads and wait for completion\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now there are dictionaries in queue (one for each thread). this few we can handle to merge \n",
    "bigram_collection = Counter({})\n",
    "for d in list(queue.queue):\n",
    "    bigram_collection+=d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_collection = dict(bigram_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted language model!\n",
    "For terms and bigrams seperately for efficiency: no locations needed\n",
    "\n",
    "Data structure will be the following:<br>\n",
    "{term : [ total_count, {index:count, ...} ],<br>\n",
    "  term : ...}\n",
    "  \n",
    "'term' for unigram model and 'bigram' for bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_terms = {}\n",
    "\n",
    "for i in range(len(FreqDistList)):\n",
    "    for term in dict(FreqDistList[i]):\n",
    "        if term in inverted_terms:\n",
    "            inverted_terms[term].update({i:dict(FreqDistList[i])[term]})\n",
    "        else:\n",
    "            inverted_terms.update({term:{i:dict(FreqDistList[i])[term]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_bigrams = {}\n",
    "\n",
    "for i in range(len(bigram_freqlist)):\n",
    "    for bigram in dict(bigram_freqlist[i]):\n",
    "        if bigram in inverted_bigrams:\n",
    "            inverted_bigrams[bigram].update({i:dict(bigram_freqlist[i])[bigram]})\n",
    "        else:\n",
    "            inverted_bigrams.update({bigram:{i:dict(bigram_freqlist[i])[bigram]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_total_terms = [sum(x.values()) for x in FreqDistList]\n",
    "song_total_bigrams = [sum(x.values()) for x in bigram_freqlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inverted_bigrams:\n",
    "    inverted_bigrams[key]=[bigram_collection[key], inverted_bigrams[key]]\n",
    "for key in inverted_terms:\n",
    "    inverted_terms[key]=[collection[key], inverted_terms[key]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
